{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOQkjm6qi37tcz4+J0vcSh3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adagio7/induction-heads/blob/main/induction_heads.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ZSlLIch77A_2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2571208a-4283-4384-8085-f001e58f9e44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformer-lens in /usr/local/lib/python3.11/dist-packages (2.15.0)\n",
            "Requirement already satisfied: accelerate>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from transformer-lens) (1.5.2)\n",
            "Requirement already satisfied: beartype<0.15.0,>=0.14.1 in /usr/local/lib/python3.11/dist-packages (from transformer-lens) (0.14.1)\n",
            "Requirement already satisfied: better-abc<0.0.4,>=0.0.3 in /usr/local/lib/python3.11/dist-packages (from transformer-lens) (0.0.3)\n",
            "Requirement already satisfied: datasets>=2.7.1 in /usr/local/lib/python3.11/dist-packages (from transformer-lens) (3.5.0)\n",
            "Requirement already satisfied: einops>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from transformer-lens) (0.8.1)\n",
            "Requirement already satisfied: fancy-einsum>=0.0.3 in /usr/local/lib/python3.11/dist-packages (from transformer-lens) (0.0.3)\n",
            "Requirement already satisfied: jaxtyping>=0.2.11 in /usr/local/lib/python3.11/dist-packages (from transformer-lens) (0.3.1)\n",
            "Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.11/dist-packages (from transformer-lens) (2.0.2)\n",
            "Requirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.11/dist-packages (from transformer-lens) (2.2.2)\n",
            "Requirement already satisfied: rich>=12.6.0 in /usr/local/lib/python3.11/dist-packages (from transformer-lens) (13.9.4)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from transformer-lens) (0.2.0)\n",
            "Requirement already satisfied: torch>=2.2 in /usr/local/lib/python3.11/dist-packages (from transformer-lens) (2.6.0+cu124)\n",
            "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.11/dist-packages (from transformer-lens) (4.67.1)\n",
            "Requirement already satisfied: transformers>=4.43 in /usr/local/lib/python3.11/dist-packages (from transformer-lens) (4.50.3)\n",
            "Requirement already satisfied: transformers-stream-generator<0.0.6,>=0.0.5 in /usr/local/lib/python3.11/dist-packages (from transformer-lens) (0.0.5)\n",
            "Requirement already satisfied: typeguard<5.0,>=4.2 in /usr/local/lib/python3.11/dist-packages (from transformer-lens) (4.4.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from transformer-lens) (4.13.0)\n",
            "Requirement already satisfied: wandb>=0.13.5 in /usr/local/lib/python3.11/dist-packages (from transformer-lens) (0.19.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.23.0->transformer-lens) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.23.0->transformer-lens) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.23.0->transformer-lens) (6.0.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.23.0->transformer-lens) (0.30.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.23.0->transformer-lens) (0.5.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.7.1->transformer-lens) (3.18.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.7.1->transformer-lens) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.7.1->transformer-lens) (0.3.8)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.7.1->transformer-lens) (2.32.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets>=2.7.1->transformer-lens) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.7.1->transformer-lens) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets>=2.7.1->transformer-lens) (2024.12.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.7.1->transformer-lens) (3.11.15)\n",
            "Requirement already satisfied: wadler-lindig>=0.1.3 in /usr/local/lib/python3.11/dist-packages (from jaxtyping>=0.2.11->transformer-lens) (0.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.5->transformer-lens) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.5->transformer-lens) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.5->transformer-lens) (2025.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=12.6.0->transformer-lens) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=12.6.0->transformer-lens) (2.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.2->transformer-lens) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2->transformer-lens) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2->transformer-lens) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2->transformer-lens) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2->transformer-lens) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2->transformer-lens) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2->transformer-lens) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2->transformer-lens) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2->transformer-lens) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2->transformer-lens) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2->transformer-lens) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2->transformer-lens) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2->transformer-lens) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2->transformer-lens) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2->transformer-lens) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2->transformer-lens) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2->transformer-lens) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.2->transformer-lens) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.43->transformer-lens) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.43->transformer-lens) (0.21.1)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb>=0.13.5->transformer-lens) (8.1.8)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb>=0.13.5->transformer-lens) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb>=0.13.5->transformer-lens) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb>=0.13.5->transformer-lens) (4.3.7)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb>=0.13.5->transformer-lens) (5.29.4)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb>=0.13.5->transformer-lens) (2.11.1)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb>=0.13.5->transformer-lens) (2.25.1)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb>=0.13.5->transformer-lens) (1.3.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb>=0.13.5->transformer-lens) (75.2.0)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb>=0.13.5->transformer-lens) (1.17.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.7.1->transformer-lens) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.7.1->transformer-lens) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.7.1->transformer-lens) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.7.1->transformer-lens) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.7.1->transformer-lens) (6.3.1)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.7.1->transformer-lens) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.7.1->transformer-lens) (1.18.3)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer-lens) (4.0.12)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=12.6.0->transformer-lens) (0.1.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb>=0.13.5->transformer-lens) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb>=0.13.5->transformer-lens) (2.33.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb>=0.13.5->transformer-lens) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.7.1->transformer-lens) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.7.1->transformer-lens) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.7.1->transformer-lens) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.7.1->transformer-lens) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.2->transformer-lens) (3.0.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer-lens) (5.0.2)\n",
            "Requirement already satisfied: circuitsvis in /usr/local/lib/python3.11/dist-packages (1.43.3)\n",
            "Requirement already satisfied: importlib-metadata>=5.1.0 in /usr/local/lib/python3.11/dist-packages (from circuitsvis) (8.6.1)\n",
            "Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.11/dist-packages (from circuitsvis) (2.0.2)\n",
            "Requirement already satisfied: torch>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from circuitsvis) (2.6.0+cu124)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata>=5.1.0->circuitsvis) (3.21.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.1->circuitsvis) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.1->circuitsvis) (4.13.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.1->circuitsvis) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.1->circuitsvis) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.1->circuitsvis) (2024.12.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.1->circuitsvis) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.1->circuitsvis) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.1->circuitsvis) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.1->circuitsvis) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.1->circuitsvis) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.1->circuitsvis) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.1->circuitsvis) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.1->circuitsvis) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.1->circuitsvis) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.1->circuitsvis) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.1->circuitsvis) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.1->circuitsvis) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.1->circuitsvis) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.1->circuitsvis) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.1->circuitsvis) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.1.1->circuitsvis) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.1.1->circuitsvis) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "%pip install transformer-lens\n",
        "%pip install circuitsvis"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import circuitsvis as cv\n",
        "import transformer_lens.utils as utils\n",
        "from transformer_lens.hook_points import HookPoint\n",
        "from transformer_lens import (\n",
        "    ActivationCache,\n",
        "    HookedTransformer,\n",
        ")\n",
        "\n",
        "# We are only really interested in model inference, not training\n",
        "torch.set_grad_enabled(False)\n",
        "device = utils.get_device()"
      ],
      "metadata": {
        "id": "RKyLQWFI-EjQ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = HookedTransformer.from_pretrained(\"gpt2-small\", device=device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VHUh3Qcp9A7O",
        "outputId": "7180fcbe-24c7-4a96-a7a4-8ad919eb9dad"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded pretrained model gpt2-small into HookedTransformer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Tom went to the store. Tom\"\n",
        "tokens = model.to_tokens(prompt)\n",
        "\n",
        "# As the name implies `remove_batch_dim` removes the first dimension (batch), as we have batch size = 1\n",
        "# However, for some reason, this doesn't seem to be working :/\n",
        "logits, cache = model.run_with_cache(tokens, remove_batch_dim=True)\n",
        "print(logits.shape)"
      ],
      "metadata": {
        "id": "pLW2xuX4_x1f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9801286d-59ef-4327-c4eb-b9dd95119bd0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 8, 50257])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Circuit Visualizers"
      ],
      "metadata": {
        "id": "oV_KUTMyVZgD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's try to visualize the attention pattern of a particular layer\n",
        "layer = 5 # Change me :D\n",
        "attention_pattern = cache[\"pattern\", layer, \"attn\"]\n",
        "gpt2_str_tokens = model.to_str_tokens(prompt)\n",
        "\n",
        "# We expect this to be [n_heads, len(prompt_tokens), len(prompt_tokens)] where the latter two dimensions are for the QK matrices\n",
        "print(f'{attention_pattern.shape=}')\n",
        "\n",
        "# Note that `attention_patterns` is deprecated, and `attention_heads` should be used instead\n",
        "# But for space, we used the old version as its more compact\n",
        "print(f'Attention Pattern for {layer=}')\n",
        "cv.attention.attention_patterns(tokens=gpt2_str_tokens, attention=attention_pattern)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        },
        "id": "qLi-kPAhBo7g",
        "outputId": "b03f23ed-2535-47d3-f7b6-a345ffc7c150"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "attention_pattern.shape=torch.Size([12, 8, 8])\n",
            "Attention Pattern for layer=5\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<circuitsvis.utils.render.RenderedHTML at 0x7cde192d1610>"
            ],
            "text/html": [
              "<div id=\"circuits-vis-ebc5c0b4-213e\" style=\"margin: 15px 0;\"/>\n",
              "    <script crossorigin type=\"module\">\n",
              "    import { render, AttentionPatterns } from \"https://unpkg.com/circuitsvis@1.43.3/dist/cdn/esm.js\";\n",
              "    render(\n",
              "      \"circuits-vis-ebc5c0b4-213e\",\n",
              "      AttentionPatterns,\n",
              "      {\"tokens\": [\"<|endoftext|>\", \"Tom\", \" went\", \" to\", \" the\", \" store\", \".\", \" Tom\"], \"attention\": [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9861912727355957, 0.013808787800371647, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9739034175872803, 0.0035533378832042217, 0.022543227300047874, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9710888266563416, 0.0004242451395839453, 0.0012083423789590597, 0.02727861888706684, 0.0, 0.0, 0.0, 0.0], [0.9657878279685974, 0.000521331443451345, 0.0007207462913356721, 0.007967882789671421, 0.025002332404255867, 0.0, 0.0, 0.0], [0.9525139927864075, 0.0019043266074731946, 0.000878571649082005, 0.01826731488108635, 0.008800312876701355, 0.01763550564646721, 0.0, 0.0], [0.9535935521125793, 0.0021793204359710217, 0.003693349426612258, 0.0018706463743001223, 0.003358552698045969, 0.0033411195036023855, 0.031963445246219635, 0.0], [0.5954970717430115, 0.0014684585621580482, 0.3559960722923279, 0.006343405228108168, 0.005979885347187519, 0.0033523808233439922, 0.0026156504172831774, 0.028747130185365677]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.999958872795105, 4.11204164265655e-05, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9991242289543152, 0.0001429644471500069, 0.0007328021456487477, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9998563528060913, 2.0668532670242712e-05, 1.1049453974010248e-07, 0.0001228817127412185, 0.0, 0.0, 0.0, 0.0], [0.9982994198799133, 4.6428682253463194e-05, 3.061751385757816e-08, 1.635831722524017e-06, 0.0016525349346920848, 0.0, 0.0, 0.0], [0.9989346861839294, 0.00015760022506583482, 3.241963213440613e-06, 6.624471211580385e-07, 4.142678517382592e-05, 0.000862395332660526, 0.0, 0.0], [0.9971206784248352, 0.0002079089463222772, 4.79415439258446e-06, 7.175831484573791e-08, 1.0990068403771147e-05, 0.00022172440367285162, 0.002433866960927844, 0.0], [0.6684593558311462, 1.0806601494550705e-05, 0.3302497863769531, 4.52999529443332e-06, 6.253775381992455e-07, 2.3607208277098835e-05, 3.966999429394491e-05, 0.0012115811696276069]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9890307784080505, 0.010969273746013641, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8065946102142334, 0.17578943073749542, 0.01761596091091633, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8630366325378418, 0.0695740208029747, 0.06264067441225052, 0.004748636856675148, 0.0, 0.0, 0.0, 0.0], [0.8479738831520081, 0.025741692632436752, 0.07434305548667908, 0.04781971871852875, 0.004121602978557348, 0.0, 0.0, 0.0], [0.7118237614631653, 0.00744611956179142, 0.11701090633869171, 0.09397601336240768, 0.059442706406116486, 0.01030044350773096, 0.0, 0.0], [0.3736415207386017, 0.03835013136267662, 0.3442347049713135, 0.11232327669858932, 0.04856052249670029, 0.07000280171632767, 0.01288698147982359, 0.0], [0.9133313894271851, 0.0031691715121269226, 0.030090752989053726, 0.012223804369568825, 0.0038000871427357197, 0.005301889963448048, 0.02270698919892311, 0.00937588233500719]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9517815113067627, 0.0482185073196888, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5826686024665833, 0.25520262122154236, 0.1621287614107132, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5182327032089233, 0.1959887444972992, 0.2515811622142792, 0.034197356551885605, 0.0, 0.0, 0.0, 0.0], [0.4754975438117981, 0.1713426411151886, 0.28215861320495605, 0.05184048414230347, 0.019160665571689606, 0.0, 0.0, 0.0], [0.5126336812973022, 0.10529457032680511, 0.24557141959667206, 0.07852480560541153, 0.029153229668736458, 0.028822297230362892, 0.0, 0.0], [0.4616863429546356, 0.17951780557632446, 0.21988043189048767, 0.04076182469725609, 0.014588480815291405, 0.04664222151041031, 0.036922886967659, 0.0], [0.45684847235679626, 0.04827619343996048, 0.13496139645576477, 0.022776629775762558, 0.011151516810059547, 0.022358564659953117, 0.25077641010284424, 0.05285079777240753]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9763398766517639, 0.02366010472178459, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9055273532867432, 0.06544210761785507, 0.029030553996562958, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7544729113578796, 0.057745885103940964, 0.12152931839227676, 0.06625185161828995, 0.0, 0.0, 0.0, 0.0], [0.5935172438621521, 0.03360708802938461, 0.10476509481668472, 0.24265643954277039, 0.025454191491007805, 0.0, 0.0, 0.0], [0.4316992163658142, 0.013968954794108868, 0.18393023312091827, 0.2034120112657547, 0.08559983223676682, 0.08138983696699142, 0.0, 0.0], [0.33087241649627686, 0.016476526856422424, 0.09674625843763351, 0.16397172212600708, 0.07589049637317657, 0.2426588535308838, 0.07338379323482513, 0.0], [0.6503201723098755, 0.0043565742671489716, 0.03213629499077797, 0.03594711422920227, 0.013863150030374527, 0.022456146776676178, 0.20626528561115265, 0.034655213356018066]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9985950589179993, 0.0014049707679077983, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9510229229927063, 0.007179867941886187, 0.041797149926424026, 0.0, 0.0, 0.0, 0.0, 0.0], [0.993691086769104, 0.004499740898609161, 0.000876310165040195, 0.0009328815503977239, 0.0, 0.0, 0.0, 0.0], [0.9888502955436707, 0.006056205369532108, 0.0003883961762767285, 0.0005883399280719459, 0.0041167098097503185, 0.0, 0.0, 0.0], [0.9813558459281921, 0.012384199537336826, 0.0007930297870188951, 0.00010871468111872673, 0.0018288943683728576, 0.0035293514374643564, 0.0, 0.0], [0.9439778327941895, 0.01798638142645359, 0.008522027172148228, 0.0009049671934917569, 0.0020496658980846405, 0.004842566326260567, 0.021716587245464325, 0.0], [0.03962968662381172, 0.00014108796312939376, 0.9590482711791992, 8.330558921443298e-05, 7.605951395817101e-05, 0.0004439133917912841, 9.049447544384748e-05, 0.0004871735000051558]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9791553616523743, 0.020844627171754837, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.49448955059051514, 0.44741830229759216, 0.05809217691421509, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6016255617141724, 0.01563359424471855, 0.35483983159065247, 0.027900977060198784, 0.0, 0.0, 0.0, 0.0], [0.7747037410736084, 0.015702659264206886, 0.08030898123979568, 0.10625618696212769, 0.023028435185551643, 0.0, 0.0, 0.0], [0.8601868152618408, 0.0036826340947300196, 0.025981584563851357, 0.025766877457499504, 0.060581568628549576, 0.023800592869520187, 0.0, 0.0], [0.8203980922698975, 0.0031860729213804007, 0.004838608670979738, 0.0026389830745756626, 0.0040438114665448666, 0.10413766652345657, 0.06075683608651161, 0.0], [0.8787071108818054, 0.0015172441489994526, 0.00493170041590929, 0.0005337295588105917, 0.0012102455366402864, 0.0056681702844798565, 0.09428992122411728, 0.013141809962689877]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.98136305809021, 0.018636934459209442, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9520580172538757, 0.006218442693352699, 0.04172346740961075, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6671671271324158, 0.0009458345011807978, 0.14603984355926514, 0.1858471781015396, 0.0, 0.0, 0.0, 0.0], [0.5684542655944824, 0.0036451935302466154, 0.2030680775642395, 0.19584150612354279, 0.02899099700152874, 0.0, 0.0, 0.0], [0.6737476587295532, 0.0027772588655352592, 0.10733017325401306, 0.16105236113071442, 0.02462828904390335, 0.030464235693216324, 0.0, 0.0], [0.6944698095321655, 0.00970445480197668, 0.03677323833107948, 0.14270347356796265, 0.020843403413891792, 0.07636848092079163, 0.019137145951390266, 0.0], [0.9607453942298889, 0.0024700972717255354, 0.0054701282642781734, 0.007638450711965561, 0.0012084504123777151, 0.006184337195008993, 0.010944748297333717, 0.005338497459888458]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9866476058959961, 0.013352383859455585, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9558767676353455, 0.02034413069486618, 0.02377903275191784, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9613451361656189, 0.007608408574014902, 0.006055895239114761, 0.0249906238168478, 0.0, 0.0, 0.0, 0.0], [0.947070300579071, 0.008336232975125313, 0.006203049793839455, 0.01027592457830906, 0.028114430606365204, 0.0, 0.0, 0.0], [0.8525976538658142, 0.016163000836968422, 0.006646701600402594, 0.007456411607563496, 0.0855821743607521, 0.03155401349067688, 0.0, 0.0], [0.840238094329834, 0.0208879504352808, 0.07376021146774292, 0.00952958781272173, 0.021414510905742645, 0.009422333911061287, 0.024747254326939583, 0.0], [0.2501346468925476, 0.0013626834843307734, 0.7021247148513794, 0.012624644674360752, 0.0073535386472940445, 0.002057640580460429, 0.004892067518085241, 0.019450007006525993]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.993831992149353, 0.006168024148792028, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9079590439796448, 0.046610888093709946, 0.04543006420135498, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9701129198074341, 0.016034003347158432, 0.008311714045703411, 0.005541327875107527, 0.0, 0.0, 0.0, 0.0], [0.9570186138153076, 0.00735444575548172, 0.010506628081202507, 0.010840367525815964, 0.014280001632869244, 0.0, 0.0, 0.0], [0.9515008926391602, 0.003112544771283865, 0.011691132560372353, 0.0094990786164999, 0.016765998676419258, 0.007430371828377247, 0.0, 0.0], [0.9134716987609863, 0.008391921408474445, 0.015812527388334274, 0.013402702286839485, 0.01522278506308794, 0.020619794726371765, 0.013078576885163784, 0.0], [0.6630747318267822, 0.036372531205415726, 0.16357897222042084, 0.022032413631677628, 0.03085106797516346, 0.01743972860276699, 0.01740877516567707, 0.04924185946583748]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9838618636131287, 0.016138140112161636, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4008946418762207, 0.48286962509155273, 0.11623577028512955, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7850653529167175, 0.08545864373445511, 0.03753421828150749, 0.09194173663854599, 0.0, 0.0, 0.0, 0.0], [0.8303447961807251, 0.06801418215036392, 0.033497121185064316, 0.050407931208610535, 0.01773586869239807, 0.0, 0.0, 0.0], [0.780532956123352, 0.02229730784893036, 0.010622355155646801, 0.03656700626015663, 0.023601971566677094, 0.1263783872127533, 0.0, 0.0], [0.371557354927063, 0.025891868397593498, 0.041688963770866394, 0.07256226241588593, 0.03824388608336449, 0.31123557686805725, 0.13882005214691162, 0.0], [0.5966308116912842, 0.11728216707706451, 0.052579257637262344, 0.04218620806932449, 0.015303228050470352, 0.010838343761861324, 0.10379425436258316, 0.06138567626476288]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9848008155822754, 0.015199253335595131, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8314119577407837, 0.020322253927588463, 0.14826583862304688, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9263221025466919, 0.007022558711469173, 0.013676763512194157, 0.052978672087192535, 0.0, 0.0, 0.0, 0.0], [0.8156698346138, 0.008559268899261951, 0.011427986435592175, 0.013439915142953396, 0.15090298652648926, 0.0, 0.0, 0.0], [0.8405339121818542, 0.013381623663008213, 0.015592032112181187, 0.04703911393880844, 0.04009125381708145, 0.04336215928196907, 0.0, 0.0], [0.5898821353912354, 0.018400857225060463, 0.025702398270368576, 0.042882099747657776, 0.039788223803043365, 0.05912245064973831, 0.2242218255996704, 0.0], [0.8223029375076294, 0.015198160894215107, 0.04973044991493225, 0.012795938178896904, 0.024605777114629745, 0.014799617230892181, 0.03211386501789093, 0.028453320264816284]]]}\n",
              "    )\n",
              "    </script>"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use `circuitvis` to visualize the attention pattern of the layer, and to identify the induction heads, we just have to find the head that seemingly pays higher attention to the previous instance of the same token.\n",
        "\n",
        "From iterating through the layers, we find that Head (5.5, 5.8) seem to correspond to induction heads.\n",
        "\n",
        "**N.B**: We also note some other pretty interesting patterns, such as Head 2.11 exclusively attending to the first token in the stream, Head 0.1 that self-attends, and a bunch of Heads (e.g. 4.11) that attend to the previous token."
      ],
      "metadata": {
        "id": "fxSwyl1xCr4A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "INDUCTION_LAYER = 5\n",
        "INDUCTION_HEAD = 8"
      ],
      "metadata": {
        "id": "aTT858c5OIVJ"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Activation Patching\n",
        "\n",
        "Now that we have our hypothesized induction heads, how can be so sure that this particular head is *neccessary* for induction? Could it be that its calculating some indirect attention that is used downstream? Essentially, we want to formalize that this particular head is neccessary and sufficient for induction to occur.\n",
        "\n",
        "How we answer this is via *activation patching*, we run an ablation on the input prompt, namely, one where induction does occur and another where it doesn't (for control, we make the prefix of the prompt the same, and only augment the final token). This technique is inspired by causal ablation studies, where we isolate the sufficient causes via interventions."
      ],
      "metadata": {
        "id": "XPM7dfFXQLRI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We introduce two prompts, one positive case of the inductive behaviour and another of negative\n",
        "clean_prompt = \"Tom went to the store. Tom\"\n",
        "corrupt_prompt = \"Tom went to the store. Sarah\"\n",
        "\n",
        "clean_tokens = model.to_tokens(clean_prompt)\n",
        "corrupt_tokens = model.to_tokens(corrupt_prompt)"
      ],
      "metadata": {
        "id": "_yvYSJypk7ls"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To do the activation patching, we first do a forward pass with the positive case\n",
        "\n",
        "We then take the attention pattern of our hypothesized head and intervened on the corrupt pass\n"
      ],
      "metadata": {
        "id": "fUsHYts0Op2h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clean_logits, clean_cache = model.run_with_cache(clean_tokens)\n",
        "corrupt_logits, corrupt_cache = model.run_with_cache(corrupt_tokens)"
      ],
      "metadata": {
        "id": "TMEd5J8wOOfu"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def patch_head_output(corrupt_cache: ActivationCache, hook: HookPoint):\n",
        "  \"\"\"\n",
        "  Applies the clean_cache activation at INDUCTION_LAYER for INDUCTION_HEAD\n",
        "  \"\"\"\n",
        "  # Find appropriate layer to get the activations\n",
        "  clean_z = clean_cache[utils.get_act_name('z', INDUCTION_LAYER)]\n",
        "  corrupt_z = corrupt_cache.clone()\n",
        "\n",
        "  # Patch the head\n",
        "  corrupt_z[:, :, INDUCTION_HEAD, :] = clean_z[:, :, INDUCTION_HEAD, :]\n",
        "\n",
        "  return corrupt_z\n"
      ],
      "metadata": {
        "id": "nRlj9fmzQScF"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the corrupt prompt but with the activation patch\n",
        "patched_logits = model.run_with_hooks(\n",
        "    corrupt_tokens,\n",
        "    fwd_hooks=[(\n",
        "        utils.get_act_name(\"z\", INDUCTION_LAYER),\n",
        "        patch_head_output\n",
        "    )]\n",
        ")"
      ],
      "metadata": {
        "id": "18LrgjdbSXNH"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using this, we can then take a metric to quanitfy the causal effect of the counterfactual."
      ],
      "metadata": {
        "id": "dYLSnjICg9Se"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We are only interested in the final generated token\n",
        "target_pos = -1"
      ],
      "metadata": {
        "id": "VQubWYj1UHk6"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the prediction logits for the clean, corrupt and patched baseline\n",
        "# Do this by extracting logits from [batch, n_ctx, n_vocab]\n",
        "clean_last_logits = clean_logits[0, target_pos, :]\n",
        "corrupt_last_logits = corrupt_logits[0, target_pos, :]\n",
        "patched_last_logits = patched_logits[0, target_pos, :]\n",
        "\n",
        "# Logits should only be for the last column, so should be vocab_size\n",
        "assert(clean_last_logits.shape == (50257,))\n",
        "assert(corrupt_last_logits.shape == (50257,))\n",
        "assert(patched_last_logits.shape == (50257,))"
      ],
      "metadata": {
        "id": "RRGy-ekBULPe"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_logit_diff(logits, correct: int, distractor: int):\n",
        "  \"\"\"\n",
        "  Returns the logits difference at the `correct` index and `distractor` index\n",
        "  \"\"\"\n",
        "  return logits[correct] - logits[distractor]\n",
        "\n",
        "def get_argmax_token(logits):\n",
        "  \"\"\"\n",
        "  Returns the string corresponding to the max logit\n",
        "  \"\"\"\n",
        "  return model.to_string(torch.argmax(logits))"
      ],
      "metadata": {
        "id": "Q9EAMAh9YMS7"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Note that GPT2 is space sensitive\n",
        "correct_token = model.to_single_token(\" went\")\n",
        "wrong_token = model.to_single_token(\" was\")"
      ],
      "metadata": {
        "id": "Ii8KmVrsahEL"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logit_diff_clean = get_logit_diff(clean_last_logits, correct_token, wrong_token)\n",
        "logit_diff_corrupt = get_logit_diff(corrupt_last_logits, correct_token, wrong_token)\n",
        "logit_diff_patched = get_logit_diff(patched_last_logits, correct_token, wrong_token)\n",
        "\n",
        "print(\"Logit diff (clean):\", logit_diff_clean.item())\n",
        "print(\"Logit diff (corrupt):\", logit_diff_corrupt.item())\n",
        "print(\"Logit diff (patched):\", logit_diff_patched.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJFy1F0daeVN",
        "outputId": "a5a4962a-0478-4679-9dfa-ebbe5747d7e2"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logit diff (clean): 0.07222270965576172\n",
            "Logit diff (corrupt): -0.44322967529296875\n",
            "Logit diff (patched): 0.2803611755371094\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Most likely token (clean):\", get_argmax_token(clean_last_logits))\n",
        "print(\"Most likely token (corrupt):\", get_argmax_token(corrupt_last_logits))\n",
        "print(\"Most likely token (patched):\", get_argmax_token(patched_last_logits))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZkxDaPMTZr7T",
        "outputId": "bc93a817-9b66-4cde-b06a-74a100c2f957"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most likely token (clean):  went\n",
            "Most likely token (corrupt):  was\n",
            "Most likely token (patched):  went\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We note the success of the activation patching method as it greatly enhances the probability of inductive behaviour (as seen by the increase between the corrupt and patched).\n",
        "\n",
        "However, we note the oddness that for the clean prompt, the logit is not very significant, which might suggest that the logit distribution is more uniform than expected. However, it still remains the most likely token for the clean logit."
      ],
      "metadata": {
        "id": "VD4QFzw2bGGv"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "71q7HWQkhYDw"
      },
      "execution_count": 56,
      "outputs": []
    }
  ]
}