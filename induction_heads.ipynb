{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOSBD6coMOkpqFU4z7/3fPe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adagio7/induction-heads/blob/main/induction_heads.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZSlLIch77A_2"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "%pip install transformer-lens\n",
        "%pip install circuitsvis"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import circuitsvis as cv\n",
        "import transformer_lens.utils as utils\n",
        "from transformer_lens import (\n",
        "    HookedTransformer\n",
        ")\n",
        "\n",
        "# We are only really interested in model inference, not training\n",
        "torch.set_grad_enabled(False)\n",
        "device = utils.get_device()"
      ],
      "metadata": {
        "id": "RKyLQWFI-EjQ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = HookedTransformer.from_pretrained(\"gpt2-small\", device=device)"
      ],
      "metadata": {
        "id": "VHUh3Qcp9A7O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Tom went to the store. Tom\"\n",
        "tokens = model.to_tokens(prompt)\n",
        "\n",
        "# As the name implies `remove_batch_dim` removes the first dimension (batch), as we have batch size = 1\n",
        "# However, for some reason, this doesn't seem to be working :/\n",
        "logits, cache = model.run_with_cache(tokens, remove_batch_dim=True)\n",
        "print(logits.shape)"
      ],
      "metadata": {
        "id": "pLW2xuX4_x1f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "860852fc-b991-43ce-bae5-88506517d820"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 8, 50257])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Circuit Visualizers"
      ],
      "metadata": {
        "id": "oV_KUTMyVZgD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's try to visualize the attention pattern of a particular layer\n",
        "layer = 8 # Change me :D\n",
        "attention_pattern = cache[\"pattern\", layer, \"attn\"]\n",
        "gpt2_str_tokens = model.to_str_tokens(prompt)\n",
        "\n",
        "# We expect this to be [n_heads, len(prompt_tokens), len(prompt_tokens)] where the latter two dimensions are for the QK vectors respectively\n",
        "print(f'{attention_pattern.shape=}')\n",
        "\n",
        "# Note that `attention_patterns` is deprecated, and `attention_heads` should be used instead\n",
        "# But for space, we used the old version as its more compact\n",
        "print(f'Attention Pattern for {layer=}')\n",
        "cv.attention.attention_patterns(tokens=gpt2_str_tokens, attention=attention_pattern)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        },
        "id": "qLi-kPAhBo7g",
        "outputId": "1cfc25b8-6608-4274-b842-9440e9388d35"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "attention_pattern.shape=torch.Size([12, 8, 8])\n",
            "Attention Pattern for layer=8\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<circuitsvis.utils.render.RenderedHTML at 0x7de4b02c7290>"
            ],
            "text/html": [
              "<div id=\"circuits-vis-63f07914-46c6\" style=\"margin: 15px 0;\"/>\n",
              "    <script crossorigin type=\"module\">\n",
              "    import { render, AttentionPatterns } from \"https://unpkg.com/circuitsvis@1.43.3/dist/cdn/esm.js\";\n",
              "    render(\n",
              "      \"circuits-vis-63f07914-46c6\",\n",
              "      AttentionPatterns,\n",
              "      {\"tokens\": [\"<|endoftext|>\", \"Tom\", \" went\", \" to\", \" the\", \" store\", \".\", \" Tom\"], \"attention\": [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9752038717269897, 0.024796154350042343, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8518649339675903, 0.03265702351927757, 0.11547807604074478, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8979287147521973, 0.010159733705222607, 0.06277415156364441, 0.029137397184967995, 0.0, 0.0, 0.0, 0.0], [0.811103880405426, 0.007125294767320156, 0.0650818943977356, 0.04082369804382324, 0.07586520910263062, 0.0, 0.0, 0.0], [0.8842151761054993, 0.010196640156209469, 0.0269335750490427, 0.021691573783755302, 0.0313914529979229, 0.025571441277861595, 0.0, 0.0], [0.8451461791992188, 0.013211953453719616, 0.030958885326981544, 0.005014718044549227, 0.03094322979450226, 0.007655163761228323, 0.06706982851028442, 0.0], [0.8578020334243774, 0.00472696078941226, 0.011225616559386253, 0.0054793041199445724, 0.04014548286795616, 0.011242276057600975, 0.05178608372807503, 0.017592303454875946]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9997712969779968, 0.00022867928782943636, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9968081116676331, 0.0012207721592858434, 0.001971085090190172, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9938796758651733, 0.0007226370507851243, 0.0005889538442716002, 0.0048087239265441895, 0.0, 0.0, 0.0, 0.0], [0.9966141581535339, 0.0006927392678335309, 0.00010931413999060169, 0.00081079569645226, 0.0017730717081576586, 0.0, 0.0, 0.0], [0.9909564852714539, 0.0010658861137926579, 0.0002721677301451564, 0.00030327195418067276, 0.0008717381278984249, 0.006530558690428734, 0.0, 0.0], [0.9127458333969116, 0.03524722158908844, 0.006529968231916428, 0.0019345700275152922, 0.008222710341215134, 0.024009596556425095, 0.011310050264000893, 0.0], [0.8825327754020691, 0.0007148858858272433, 0.08403566479682922, 0.014965422451496124, 0.004635932389646769, 0.0010376699501648545, 0.008294261991977692, 0.0037833894602954388]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9925491809844971, 0.00745079480111599, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8672263622283936, 0.03954915702342987, 0.0932244136929512, 0.0, 0.0, 0.0, 0.0, 0.0], [0.84221351146698, 0.06995592266321182, 0.06722521781921387, 0.020605364814400673, 0.0, 0.0, 0.0, 0.0], [0.8400225639343262, 0.04371378570795059, 0.06903696805238724, 0.025989264249801636, 0.021237455308437347, 0.0, 0.0, 0.0], [0.8424279093742371, 0.020791692659258842, 0.04324791207909584, 0.0278839860111475, 0.014787135645747185, 0.05086135491728783, 0.0, 0.0], [0.5350959300994873, 0.07327007502317429, 0.048432935029268265, 0.03339596837759018, 0.018428750336170197, 0.08074269443750381, 0.21063368022441864, 0.0], [0.643734872341156, 0.035444870591163635, 0.03131536394357681, 0.01924801804125309, 0.010244198143482208, 0.05307939648628235, 0.14635631442070007, 0.06057688966393471]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9897060394287109, 0.010293904691934586, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.990139901638031, 0.004374090116471052, 0.005486013367772102, 0.0, 0.0, 0.0, 0.0, 0.0], [0.95567786693573, 0.022067230194807053, 0.01561727374792099, 0.00663759745657444, 0.0, 0.0, 0.0, 0.0], [0.9735756516456604, 0.0053166779689490795, 0.006661882624030113, 0.004010208882391453, 0.010435642674565315, 0.0, 0.0, 0.0], [0.8300623893737793, 0.005158362910151482, 0.007694273721426725, 0.008653982542455196, 0.015332273207604885, 0.133098766207695, 0.0, 0.0], [0.5640069246292114, 0.04763459041714668, 0.12032622843980789, 0.027334269136190414, 0.03455003723502159, 0.0664653331041336, 0.1396826058626175, 0.0], [0.8370354771614075, 0.010618172585964203, 0.023203862830996513, 0.015225518494844437, 0.016147630289196968, 0.018547244369983673, 0.06788276880979538, 0.01133929006755352]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9828238487243652, 0.017176087945699692, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9673491716384888, 0.010608093813061714, 0.022042736411094666, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8999897241592407, 0.03663449361920357, 0.028824150562286377, 0.03455164283514023, 0.0, 0.0, 0.0, 0.0], [0.7660080790519714, 0.03797134384512901, 0.023259246721863747, 0.12519152462482452, 0.047569744288921356, 0.0, 0.0, 0.0], [0.7908267974853516, 0.004370462149381638, 0.011699081398546696, 0.06933750212192535, 0.06162845715880394, 0.06213773787021637, 0.0, 0.0], [0.925752580165863, 0.006006922572851181, 0.0025123185478150845, 0.006311130244284868, 0.007791155017912388, 0.02465669810771942, 0.02696917951107025, 0.0], [0.4832004904747009, 0.003384880954399705, 0.004667630884796381, 0.005728608928620815, 0.00956350564956665, 0.0637928694486618, 0.411041796207428, 0.018620170652866364]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9651970863342285, 0.034802958369255066, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8022944927215576, 0.12651431560516357, 0.07119116932153702, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6363726854324341, 0.06241705268621445, 0.26813971996307373, 0.03307056799530983, 0.0, 0.0, 0.0, 0.0], [0.70917147397995, 0.049207188189029694, 0.13348066806793213, 0.08595389872789383, 0.022186724469065666, 0.0, 0.0, 0.0], [0.5572757124900818, 0.029547419399023056, 0.1560201197862625, 0.11590766161680222, 0.07490575313568115, 0.06634337455034256, 0.0, 0.0], [0.6194522380828857, 0.03524976968765259, 0.11822215467691422, 0.029109904542565346, 0.022400015965104103, 0.06896010041236877, 0.10660585016012192, 0.0], [0.5137115716934204, 0.01190855074673891, 0.008266973309218884, 0.003807604545727372, 0.002128828316926956, 0.020818378776311874, 0.37917545437812805, 0.06018258258700371]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9878690838813782, 0.01213085651397705, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8906306624412537, 0.05165042728185654, 0.057718921452760696, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9214395880699158, 0.054603613913059235, 0.019019445404410362, 0.004937446676194668, 0.0, 0.0, 0.0, 0.0], [0.9735183715820312, 0.008989959955215454, 0.010134261101484299, 0.0035523748956620693, 0.0038050622679293156, 0.0, 0.0, 0.0], [0.9262813925743103, 0.01151224970817566, 0.0415637269616127, 0.008380386047065258, 0.003784728003665805, 0.008477565832436085, 0.0, 0.0], [0.2642389237880707, 0.43093445897102356, 0.18409143388271332, 0.04380245879292488, 0.0231204591691494, 0.01781797781586647, 0.03599429130554199, 0.0], [0.44362613558769226, 0.007866847328841686, 0.10607054084539413, 0.0830579474568367, 0.02673448994755745, 0.003404820105060935, 0.31321507692337036, 0.016024144366383553]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.974998950958252, 0.02500111237168312, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9646357893943787, 0.025542087852954865, 0.009822038002312183, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9084787964820862, 0.0044845351949334145, 0.05117977038025856, 0.03585691377520561, 0.0, 0.0, 0.0, 0.0], [0.370466411113739, 0.0033495656680315733, 0.05066071078181267, 0.5424715876579285, 0.03305170312523842, 0.0, 0.0, 0.0], [0.6709920167922974, 0.010463358834385872, 0.03333760052919388, 0.10543669015169144, 0.1539703607559204, 0.025799937546253204, 0.0, 0.0], [0.9514075517654419, 0.009157443419098854, 0.004157747142016888, 0.004271826706826687, 0.006856725085526705, 0.01647907681763172, 0.007669653277844191, 0.0], [0.6506494283676147, 0.004085896536707878, 0.001429143943823874, 0.0016548442654311657, 0.0015504714101552963, 0.012010997161269188, 0.31607329845428467, 0.012545865960419178]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9910891056060791, 0.008910886943340302, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8733305335044861, 0.04025695472955704, 0.08641255646944046, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8258646726608276, 0.060112521052360535, 0.04408037289977074, 0.06994248926639557, 0.0, 0.0, 0.0, 0.0], [0.7840907573699951, 0.037328001111745834, 0.02959459088742733, 0.08872909098863602, 0.06025762856006622, 0.0, 0.0, 0.0], [0.8766655921936035, 0.0037624454125761986, 0.00522961001843214, 0.03525206446647644, 0.020364932715892792, 0.058725375682115555, 0.0, 0.0], [0.3975365459918976, 0.04401082918047905, 0.0147101366892457, 0.07342569530010223, 0.031780123710632324, 0.30651628971099854, 0.13202039897441864, 0.0], [0.29901865124702454, 0.0028555879835039377, 0.01188048254698515, 0.07647088170051575, 0.049966130405664444, 0.25340816378593445, 0.3023437261581421, 0.004056377802044153]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9845119714736938, 0.015487991273403168, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9668713808059692, 0.012389986775815487, 0.020738661289215088, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9154691100120544, 0.0136760538443923, 0.024292461574077606, 0.04656244069337845, 0.0, 0.0, 0.0, 0.0], [0.8983652591705322, 0.007178321480751038, 0.014893223531544209, 0.044959716498851776, 0.03460344299674034, 0.0, 0.0, 0.0], [0.8155626058578491, 0.003075185464695096, 0.009724295698106289, 0.03507547453045845, 0.03362136706709862, 0.1029411032795906, 0.0, 0.0], [0.7151755094528198, 0.015784380957484245, 0.02117636241018772, 0.05593026056885719, 0.04325170814990997, 0.06954304128885269, 0.07913881540298462, 0.0], [0.7747034430503845, 0.006516674067825079, 0.019530463963747025, 0.045477189123630524, 0.022419266402721405, 0.040029410272836685, 0.06748778373003006, 0.023835843428969383]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.956550121307373, 0.04344986006617546, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9563517570495605, 0.020315449684858322, 0.0233328677713871, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8258948922157288, 0.08428338170051575, 0.03454269841313362, 0.055278975516557693, 0.0, 0.0, 0.0, 0.0], [0.7911102771759033, 0.028135940432548523, 0.02010536380112171, 0.05386410653591156, 0.10678432881832123, 0.0, 0.0, 0.0], [0.7932394742965698, 0.014855772256851196, 0.024724643677473068, 0.046731386333703995, 0.0566411167383194, 0.06380762904882431, 0.0, 0.0], [0.5560715198516846, 0.1591956913471222, 0.07895586639642715, 0.037112802267074585, 0.038128312677145004, 0.035347722470760345, 0.09518810361623764, 0.0], [0.5920518636703491, 0.008560596033930779, 0.08135416358709335, 0.04363071173429489, 0.030179621651768684, 0.03655128926038742, 0.14473357796669006, 0.06293824315071106]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9669878482818604, 0.03301212564110756, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9870883822441101, 0.01000518910586834, 0.002906440757215023, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9521700739860535, 0.034005384892225266, 0.006826871540397406, 0.006997669115662575, 0.0, 0.0, 0.0, 0.0], [0.9334625601768494, 0.030198706313967705, 0.012435393407940865, 0.015206977725028992, 0.008696413598954678, 0.0, 0.0, 0.0], [0.9279595613479614, 0.013878083787858486, 0.006906125228852034, 0.008106676861643791, 0.014482424594461918, 0.028667129576206207, 0.0, 0.0], [0.7408934831619263, 0.07259680330753326, 0.04082115739583969, 0.018598509952425957, 0.023993561044335365, 0.04886142164468765, 0.05423509329557419, 0.0], [0.8651447892189026, 0.00797116570174694, 0.023555723950266838, 0.02947494201362133, 0.011503673158586025, 0.025210702791810036, 0.034031786024570465, 0.003107324941083789]]]}\n",
              "    )\n",
              "    </script>"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use circuitvis to visualize the attention pattern of the layer, and to identify the induction heads, we just have to find the head that seemingly pays higher attention to the previous instance of the same token.\n",
        "\n",
        "From iterating through the layers, we find that Head 4.1, seem to correspond to induction heads.\n",
        "\n",
        "**N.B**: We also note some other pretty interesting patterns, such as Head 2.11 exclusively attending to the first token in the stream, Head 0.1 that self-attends, and a bunch of Heads (e.g. 4.11) that attend to the previous token."
      ],
      "metadata": {
        "id": "fxSwyl1xCr4A"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XPM7dfFXQLRI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}